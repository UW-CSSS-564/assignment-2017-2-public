---
title: 'STAT/CSSS 564: Assignment 2'
author: Patrick Liu
output: html_document
date: "April 17, 2017"
---

## Instructions

1. Fork this repository to your account
2. Edit the file `solutions.Rmd` with your solutions to the problems.
3. Submit a pull request to have it graded. Include either or both a HTML and PDF file.

For updates and questions follow the Slack channel: [#assignment2](https://uwcsss564.slack.com/messages/C516ETYBY).

This assignment will require the following R packages:
```{r, message=FALSE}
library(rstan)
library(data.table)
```

## How much does the prior influence the posterior?

In this problem, we aim for a greater understanding of the influence of a normal prior on inference about the mean of a normal distribution.

Suppose you observe $n$ single data points which you model as coming from a normal distribution. Consider the case in which the mean of the distribution is $\mu$ (unknown), but the variance, $\sigma^2$, is known:
$$
x | \mu \sim N(\mu, \sigma^2)
$$

You provide a prior distribution for $\mu$,
$$
\mu \sim N(\mu_0, \sigma_0^2)
$$

A normal prior for $\mu$ is [conjugate](https://en.wikipedia.org/wiki/Conjugate_prior) to a normal likelihood, meaning that the posterior distribution is also normal:
$$
\mu | x \sim N(\frac{\frac{1}{\sigma_0^2} \mu_0 + \frac{n}{\sigma^2} \bar{x}}{\frac{1}{\sigma_0^2} + \frac{n}{\sigma^2}}, \frac{1}{\sigma_0^2} + \frac{n}{\sigma^2})
$$

**Q1** In your own words, what is a conjugate prior? What are the advantages and disadvantages of such a prior?
Suppose that instead of a normal prior for $\mu$, a non-conjugate distribution such as the [Laplace distribution](https://en.wikipedia.org/wiki/Laplace_distribution) were used. Write down the posterior density function in this case. Why might a normal prior be preferred?

A conjugate prior occurs when the prior and posterior distributions are in the same family--in this case the prior distribution is called a conjugate prior. This is computationally beneficial, but can be overly restrictive when describing the prior. If a non-conjugate distribution is used, then the posterior distribution will no longer be normal--again this is less computationally easy.

**Q2** Describe the effects of the following on inference about $\mu$:

* Lots of data vs. not much data

Lots of data increases $n$, which increases the weight on the sample mean relative to the prior mean; it also decreases the variance on $\mu$

* Strong prior information vs. weak prior information

Strong prior information will have a small $\sigma_0^2$, which increases the influence of the prior and decreases the variance on $\mu$

* Lots vs. little noise in the data

Lots of noise in the data increases $\sigma^2$, which reduces the influence of the data on the mean and increases the variance of the posterior, makin

* Data that is incompatible with the prior vs. data that is compatible with the prior

If the prior and data are incompatible, then the posterior distribution will be a mean that is in between the two--depending on how they are weighted, the distribution could look unlike either the prior or data.

A normal prior with finite variance necessarily contributes information to the model; the question is how much. We now investigate a way of understanding the information in a normal prior in terms of implicit additional observed data.

**Q3** The data in `data/pr2data.csv` were generated from a normal distribution with a certain mean and variance. First, fit a linear model with just an intercept and report the estimated mean and variance, as well as a centered 97% confidence interval for $\mu$. Interpret the confidence interval appropriately.

```{r}
## Load data
df <- fread("C:/Users/pyliu/Desktop/git/assignment2/data/pr2data.csv")

## Fit linear model
mod <- lm(x~1, data=df)
mod.ci <- confint(mod, level=0.97)

message(paste0("The mean estimate is : ", summary(mod)$coefficients[1]))
message(paste0("The variance estimate is : ", summary(mod)$coefficients[2]^2))
message(paste0("The 97% CI for the mean estimate is : ", mod.ci[1], "-", mod.ci[2]))

```

The 97% CI suggests that under repeated trials, the true mean will be contained in the interval 97% of the time.

**Q4** Now you're told that the variance of the normal distribution that generated these data is 100 (this should be no surprise if you've done the previous question). Fit a Bayesian model for the mean assuming a normal likelihood with standard deviation 10, and with prior $N(10, 0.1^2)$. (In the usual notation for the normal model, the second parameter is the variance; but in *Stan*, the second parameter is the standard deviation, because that is unfortunately the convention in *R*. So here, the standard deviation is $0.1$ and the variance is $0.01$.) Would you characterize this prior as "highly informative," "modestly informative," or "uninformative," and why? Provide the same output (estimate, variance/standard deviation of estimate, confidence interval) you did in **Q3**, and interpret the 97% confidence interval appropriately (hint: use the *print* function for Stan models).

```{r}
## Fit the data again with a prior of N(10, 0.01)
data <- list(N=nrow(df),x=df$x)
fit <- stan(file="data.stan", data=data, iter=1000, chain=2)
fit.beta <- extract(fit)
mean <- mean(fit.beta$beta)
ci <- quantile(fit.beta$beta, c(0.015, 0.985))
print(fit)

```

In this case, there is a 97% chance that the true mean is within the credible interval (9.3, 10.7). The prior is very informative--using it results in a standard deviation of 0.09 for the posterior distribution, which is much smlaler than the SE for the linear model.

**Q5** Now, generate $10^4$ independent observations from a $N(10, 10^2)$ distribution, center them to have a mean of 10 (this is cheating a bit, we fully admit), concatenate them to the data from `data/pr2data.csv`, and fit both a standard linear model with just a mean intercept and a Bayesian model with normal likelihood and an improper flat prior. Provide the same outputs as before (no need to interpret the confidence intervals again, though).

```{r}
## Generate observations and append to df
obs <- rnorm(n=10^4, mean=10, sd=10)
df.c <- rbind(df, obs)

## Fit linear model
mod <- lm(x~1, data=df.c)
mod.ci <- confint(mod, level=0.97)
message(paste0("Linear | The mean estimate is : ", summary(mod)$coefficients[1]))
message(paste0("Linear | The variance estimate is : ", summary(mod)$coefficients[2]^2))
message(paste0("Linear | The 97% CI for the mean estimate is : ", mod.ci[1], "-", mod.ci[2]))

## Fit bayesian model
data <- list(N=nrow(df.c),x=df.c$x)
fit <- stan(file="data2.stan", data=data, iter=1000, chain=2)
fit.beta <- extract(fit)
mean <- mean(fit.beta$beta)
ci <- quantile(fit.beta$beta, c(0.015, 0.985))

message(paste0("Bayesian | The mean estimate is : ", mean))
message(paste0("Bayesian | The 97% credible interval is : ", ci[1], "-", ci[2]))

print(fit)

```

**Q6** What do you think is going on here? How can we interpret the prior in terms of a number of additional data points drawn from a distribution? Relate this distribution back to the formula for the posterior distribution $\mu | x$, given before **Q1**. 

Comparing between the 97% CI in Q4 and Q5, we see that the strong prior we gave is as informative as giving the model an additional 10 times the amount of data. Relating this back to the formula, this example is showing that reducing $\sigma_0^2$ is analogous to increasing the $n$ on the prior. 

**Q7** What simplifying assumptions were made in this problem for illustrative purposes, and what do you think might happen to inference about $\mu$ (the estimation of the mean and the standard error of that mean) if those assumptions were relaxed?

The simplifying assumption made in this problem was that we knew the variance $\sigma^2$ for the data-generating process. If we instead allowed it to draw from a distribution, we'd expect for the uncertainty around the mean to increase. 




